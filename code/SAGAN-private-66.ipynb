{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genmodel', 'generative-dog-images', 'gendog']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../input/gendog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./utils.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('../input/gendog/train.py', './train.py')\n",
    "copyfile('../input/gendog/parameters.py', './parameters.py')\n",
    "copyfile('../input/gendog/self_atten_gan.py', './self_atten_gan.py')\n",
    "copyfile('../input/gendog/layers.py', './layers.py')\n",
    "copyfile('../input/gendog/data_processing.py', './data_processing.py')\n",
    "copyfile('../input/gendog/utils.py', './utils.py')\n",
    "copyfile('../input/gendog/generate_samples.py', './generate_samples.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__notebook_source__.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'parameters.py',\n",
       " 'train.py',\n",
       " 'self_atten_gan.py',\n",
       " 'data_processing.py',\n",
       " 'layers.py',\n",
       " 'utils.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python3 train.py --dataroot='../input/generative-dog-images/' --num_epoches=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gen_model.pth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../sagan_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./generate_samples.py'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyfile('../input/gendog/generate_samples.py', './generate_samples.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python3 generate_samples.py --num_sample_images=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_00041.png',\n",
       " 'image_00015.png',\n",
       " 'image_00023.png',\n",
       " 'image_00094.png',\n",
       " 'image_00036.png',\n",
       " 'image_00084.png',\n",
       " 'image_00073.png',\n",
       " 'image_00046.png',\n",
       " 'image_00045.png',\n",
       " 'image_00096.png',\n",
       " 'image_00018.png',\n",
       " 'image_00048.png',\n",
       " 'image_00016.png',\n",
       " 'image_00035.png',\n",
       " 'image_00017.png',\n",
       " 'image_00087.png',\n",
       " 'image_00026.png',\n",
       " 'image_00075.png',\n",
       " 'image_00080.png',\n",
       " 'image_00098.png',\n",
       " 'image_00001.png',\n",
       " 'image_00089.png',\n",
       " 'image_00012.png',\n",
       " 'image_00063.png',\n",
       " 'image_00058.png',\n",
       " 'image_00031.png',\n",
       " 'image_00078.png',\n",
       " 'image_00029.png',\n",
       " 'image_00054.png',\n",
       " 'image_00057.png',\n",
       " 'image_00051.png',\n",
       " 'image_00030.png',\n",
       " 'image_00083.png',\n",
       " 'image_00009.png',\n",
       " 'image_00055.png',\n",
       " 'image_00021.png',\n",
       " 'image_00044.png',\n",
       " 'image_00008.png',\n",
       " 'image_00088.png',\n",
       " 'image_00040.png',\n",
       " 'image_00028.png',\n",
       " 'image_00003.png',\n",
       " 'image_00000.png',\n",
       " 'image_00002.png',\n",
       " 'image_00027.png',\n",
       " 'image_00077.png',\n",
       " 'image_00037.png',\n",
       " 'image_00092.png',\n",
       " 'image_00093.png',\n",
       " 'image_00056.png',\n",
       " 'image_00049.png',\n",
       " 'image_00025.png',\n",
       " 'image_00019.png',\n",
       " 'image_00005.png',\n",
       " 'image_00076.png',\n",
       " 'image_00081.png',\n",
       " 'image_00053.png',\n",
       " 'image_00061.png',\n",
       " 'image_00060.png',\n",
       " 'image_00065.png',\n",
       " 'image_00070.png',\n",
       " 'image_00095.png',\n",
       " 'image_00097.png',\n",
       " 'image_00067.png',\n",
       " 'image_00085.png',\n",
       " 'image_00039.png',\n",
       " 'image_00007.png',\n",
       " 'image_00052.png',\n",
       " 'image_00004.png',\n",
       " 'image_00024.png',\n",
       " 'image_00034.png',\n",
       " 'image_00020.png',\n",
       " 'image_00074.png',\n",
       " 'image_00047.png',\n",
       " 'image_00062.png',\n",
       " 'image_00043.png',\n",
       " 'image_00068.png',\n",
       " 'image_00072.png',\n",
       " 'image_00014.png',\n",
       " 'image_00071.png',\n",
       " 'image_00099.png',\n",
       " 'image_00010.png',\n",
       " 'image_00086.png',\n",
       " 'image_00064.png',\n",
       " 'image_00032.png',\n",
       " 'image_00069.png',\n",
       " 'image_00022.png',\n",
       " 'image_00082.png',\n",
       " 'image_00033.png',\n",
       " 'image_00038.png',\n",
       " 'image_00091.png',\n",
       " 'image_00013.png',\n",
       " 'image_00059.png',\n",
       " 'image_00011.png',\n",
       " 'image_00079.png',\n",
       " 'image_00066.png',\n",
       " 'image_00090.png',\n",
       " 'image_00006.png',\n",
       " 'image_00050.png',\n",
       " 'image_00042.png']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../output_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "import data_processing\n",
    "import self_atten_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args(object):\n",
    "    dataroot: str = '../input/generative-dog-images/'    \n",
    "    sample_images_path: str = '../output_images/'\n",
    "    submission_dir: str = './submission/'    \n",
    "    save_model_dir: str = './sagan_models/'\n",
    "    batch_size: int = 32\n",
    "    d_lr: float = 0.0004\n",
    "    g_lr: float = 0.0002\n",
    "    beta1: float = 0.0\n",
    "    beta2: float = 0.999\n",
    "    adv_loss: str = 'dcgan'\n",
    "    z_dim: int = 180\n",
    "    g_conv_dim: int = 64\n",
    "    d_conv_dim: int = 64\n",
    "    imsize: int = 64\n",
    "    num_epoches: int = 10\n",
    "    real_label_value: float = 0.8\n",
    "    fake_label_value: float = 0\n",
    "    num_of_classes: int = 120\n",
    "    num_sample_images: int = 10000\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model, gen_losses, dis_losses = train.train_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_dir = Path(args.save_model_dir)\n",
    "gen_model_dir.mkdir(exist_ok=True)\n",
    "torch.save(gen_model.state_dict(), os.path.join(args.save_model_dir, 'gen_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_samples\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python3 generate_samples.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import self_atten_gan\n",
    "import os\n",
    "\n",
    "seed = 1234\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "gen_model = self_atten_gan.Generator(args.z_dim, args.g_conv_dim, args.num_of_classes).to(device)\n",
    "gen_model.load_state_dict(torch.load(os.path.join(args.save_model_dir, 'gen_model.pth')))\n",
    "generate_samples.save_images(generator_model=gen_model,\n",
    "                sample_images_path=args.sample_images_path,\n",
    "                submission_dir=args.submission_dir,\n",
    "                z_dim=args.z_dim,\n",
    "                num_of_classes=args.num_of_classes,\n",
    "                num_images=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed = 1234\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_processing.get_train_dataset(args.dataroot, args.imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = len(set(train_dataset.classes))\n",
    "train_dataloader = data_processing.get_dataloader(train_dataset, args.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, (img, breed_idx) in enumerate(zip(x[0][:32], x[1][:32])):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    img = img.numpy().transpose(1, 2, 0)\n",
    "    #img_class = breeds[breed_idx]\n",
    "    #plt.title(img_class)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "### self.build_models()\n",
    "G = self_atten_gan.Generator(args.z_dim, args.g_conv_dim, num_of_classes).to(device)\n",
    "D = self_atten_gan.Discriminator(args.d_conv_dim, num_of_classes).to(device)\n",
    "\n",
    "G_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), args.g_lr, [args.beta1, args.beta2])\n",
    "D_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), args.d_lr, [args.beta1, args.beta2])\n",
    "\n",
    "lr_schedulerG = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(G_optimizer, T_0=args.num_epoches//10, eta_min=0.00001)\n",
    "lr_schedulerD = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(D_optimizer, T_0=args.num_epoches//10, eta_min=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.adv_loss == 'dcgan':\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "cudnn.benchmark = True\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "label = torch.full((args.batch_size,), 1, device=device)\n",
    "ones = torch.full((args.batch_size,), 1, device=device)\n",
    "\n",
    "gen_losses = []\n",
    "dis_losses = []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(args.num_epoches):\n",
    "        \n",
    "    for i, (real_images, dog_labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        \n",
    "        real_images = real_images.to(device)\n",
    "        dog_labels = torch.tensor(dog_labels, device=device)\n",
    "                \n",
    "        # =================================== TRAIN D ============================== #\n",
    "        G_optimizer.zero_grad()\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # TRAIN with REAL        \n",
    "        # Get D output for real images & real labels\n",
    "        d_out_real = D(real_images, dog_labels)\n",
    "        \n",
    "        \n",
    "        # Compute D loss with real images & real_labels\n",
    "        label.fill_(args.real_label_value) +  np.random.uniform(-0.1, 0.1)\n",
    "        d_loss_real = criterion(torch.sigmoid(d_out_real), label)\n",
    "        \n",
    "        # Backward\n",
    "        d_loss_real.backward()\n",
    "        \n",
    "        # Train with FAKE        \n",
    "        # Create random noise\n",
    "        z = torch.randn(args.batch_size, args.z_dim, device=device)\n",
    "        \n",
    "        # Generate fake images for same dog labels\n",
    "        fake_images = G(z, dog_labels)\n",
    "        \n",
    "        # Get D output for fake images & same dog labels\n",
    "        d_out_fake = D(fake_images.detach(), dog_labels)\n",
    "        \n",
    "        # Compute D loss with fake images & real labels\n",
    "        label.fill_(args.fake_label_value) + np.random.uniform(0, 0.2)\n",
    "        d_loss_fake = criterion(torch.sigmoid(d_out_fake), label)\n",
    "        \n",
    "        # Backward\n",
    "        d_loss_fake.backward()\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # ====================================== TRAIN G ============================== #\n",
    "        G.zero_grad()\n",
    "        \n",
    "        # Get D output for fake images & same dog labels\n",
    "        d_out_fake = D(fake_images, dog_labels)\n",
    "        \n",
    "        # Compute G loss with fake images & dog_labels\n",
    "        label.fill_(args.real_label_value)\n",
    "        g_loss = criterion(torch.sigmoid(d_out_fake), label)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        G_optimizer.step()\n",
    "        \n",
    "        gen_losses.append(g_loss.item())\n",
    "        dis_losses.append(d_loss.item())\n",
    "                \n",
    "        lr_schedulerG.step(epoch)\n",
    "        lr_schedulerD.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(gen_losses,label=\"G\")\n",
    "plt.plot(dis_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "gen_model_dir = Path(args.save_model_dir)\n",
    "gen_model_dir.mkdir(exist_ok=True)\n",
    "torch.save(G.state_dict(), os.path.join(args.save_model_dir, 'gen_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = self_atten_gan.Generator(args.z_dim, args.g_conv_dim, num_of_classes).to(device)\n",
    "gen_model.load_state_dict(torch.load(os.path.join(args.save_model_dir, 'gen_model.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(generator_model=gen_model, \n",
    "            sample_images_path=args.sample_images_path, \n",
    "            submission_dir=args.submission_dir, \n",
    "            z_dim=args.z_dim, \n",
    "            num_images=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(args.sample_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(args.submission_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "import torchvision.datasets as dse\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from PIL import Image \n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Training Parameter Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args(object):\n",
    "    dataroot: str = '../input/'    \n",
    "    sample_images_path: str = '../output_images/'\n",
    "    submission_dir: str = './submission/'    \n",
    "    batch_size: int = 32\n",
    "    d_lr: float = 0.0004\n",
    "    g_lr: float = 0.0002\n",
    "    beta1: float = 0.0\n",
    "    beta2: float = 0.999\n",
    "    adv_loss: str = 'dcgan'\n",
    "    z_dim: int = 180\n",
    "    g_conv_dim: int = 64\n",
    "    d_conv_dim: int = 64\n",
    "    imsize: int = 64\n",
    "    num_epoches: int = 10\n",
    "    real_label_value: float = 0.8\n",
    "    fake_label_value: float = 0\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seed Everything**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed = 1234\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogDataset(Dataset):\n",
    "    def __init__(self, dataroot, transform=None):\n",
    "        self.images, self.classes = self.crop_images(dataroot)                \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        img_class = self.classes[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return (img, img_class)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def crop_images(self, dataroot):\n",
    "        images = []\n",
    "        classes = []\n",
    "        breeds = os.listdir(dataroot + 'annotation/Annotation/') \n",
    "        for breed_idx, breed in enumerate(breeds):\n",
    "            for dog in os.listdir(dataroot + 'annotation/Annotation/'+ breed):\n",
    "                try: \n",
    "                    img = Image.open(dataroot + 'all-dogs/all-dogs/' + dog +'.jpg') \n",
    "                except: \n",
    "                    continue           \n",
    "                tree = ET.parse(dataroot + 'annotation/Annotation/'+ breed + '/'+ dog)\n",
    "                root = tree.getroot()\n",
    "                objects = root.findall('object')\n",
    "                if len(objects) == 1:\n",
    "                    bndbox = list(objects)[0].find('bndbox') \n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    box_sz = np.min((xmax - xmin, ymax - ymin))\n",
    "                    \n",
    "                    image_w = img.size[0]\n",
    "                    image_h = img.size[1]\n",
    "                    image_sz = np.min((image_w,image_h))\n",
    "                    if box_sz / image_sz >= 0.75:\n",
    "                        a=0; b=0\n",
    "                        if image_w < image_h: b = (image_h-image_sz) // 2\n",
    "                        else: a = (image_w-image_sz) // 2\n",
    "                        img2 = img.crop((0+a, 0+b, image_sz+a, image_sz+b))\n",
    "                        img2 = img2.resize((64,64), Image.ANTIALIAS)   \n",
    "                        images.append(img2)\n",
    "                        classes.append(breed_idx)\n",
    "                    else:\n",
    "                        img2 = img.crop((xmin, ymin, xmin+box_sz, ymin+box_sz))\n",
    "                        img2 = img2.resize((64,64), Image.ANTIALIAS)\n",
    "                        images.append(img2)\n",
    "                        classes.append(breed_idx)\n",
    "                else:\n",
    "                    for o in objects:\n",
    "                        bndbox = o.find('bndbox') \n",
    "                        xmin = int(bndbox.find('xmin').text)\n",
    "                        ymin = int(bndbox.find('ymin').text)\n",
    "                        xmax = int(bndbox.find('xmax').text)\n",
    "                        ymax = int(bndbox.find('ymax').text)\n",
    "                        w = np.min((xmax - xmin, ymax - ymin))\n",
    "                        img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n",
    "                        img2 = img2.resize((64,64), Image.ANTIALIAS)\n",
    "                        images.append(img2)\n",
    "                        classes.append(breed_idx)\n",
    "                    \n",
    "        return images, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_transforms = [transforms.RandomRotation(degrees=5)]\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(args.imsize),\n",
    "                                transforms.CenterCrop(args.imsize),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomApply(random_transforms, p=0.3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                               ])\n",
    "\n",
    "train_dataset = DogDataset(dataroot=args.dataroot, transform=transform)\n",
    "num_of_classes = len(set(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x = next(iter(train_dataloader))\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, (img, breed_idx) in enumerate(zip(x[0][:32], x[1][:32])):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    img = img.numpy().transpose(1, 2, 0)\n",
    "    img_class = breeds[breed_idx]\n",
    "    plt.title(img_class)\n",
    "    plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Customer Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snlinear(in_features, out_features):\n",
    "    return spectral_norm(nn.Linear(in_features=in_features, out_features=out_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_embedding(num_embeddings, embedding_dim):\n",
    "    return spectral_norm(nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(Self_Attn, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_attn = snconv2d(in_channels=in_channels//2, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        self.sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps(B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        _, ch, h, w = x.size()\n",
    "        # Theta path\n",
    "        theta = self.snconv1x1_theta(x)\n",
    "        theta = theta.view(-1, ch//8, h*w)\n",
    "        # Phi path\n",
    "        phi = self.snconv1x1_phi(x)\n",
    "        phi = self.maxpool(phi)\n",
    "        phi = phi.view(-1, ch//8, h*w//4)\n",
    "        # Attn map\n",
    "        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n",
    "        attn = self.softmax(attn)\n",
    "        # g path\n",
    "        g = self.snconv1x1_g(x)\n",
    "        g = self.maxpool(g)\n",
    "        g = g.view(-1, ch//2, h*w//4)\n",
    "        # Attn_g\n",
    "        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n",
    "        attn_g = attn_g.view(-1, ch//2, h, w)\n",
    "        attn_g = self.snconv1x1_attn(attn_g)\n",
    "        # Out\n",
    "        out = x + self.sigma*attn_g\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = nn.BatchNorm2d(num_features, momentum=0.001, affine=False)\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
    "        self.embed.weight.data[:, :num_features].fill_(1.)  # Initialize scale to 1\n",
    "        self.embed.weight.data[:, num_features:].zero_()  # Initialize bias at 0\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.bn(x)\n",
    "        gamma, beta = self.embed(y).chunk(2, 1)\n",
    "        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(GenBlock, self).__init__()\n",
    "        self.cond_bn1 = ConditionalBatchNorm2d(in_channels, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        self.snconv2d1 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.cond_bn2 = ConditionalBatchNorm2d(out_channels, num_classes)\n",
    "        self.snconv2d2 = snconv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.snconv2d0 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x0 = x\n",
    "\n",
    "        x = self.cond_bn1(x, labels)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest') # upsample\n",
    "        #x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True) # upsample\n",
    "        x = self.snconv2d1(x)\n",
    "        x = self.cond_bn2(x, labels)\n",
    "        x = self.relu(x)\n",
    "        x = self.snconv2d2(x)\n",
    "\n",
    "        x0 = F.interpolate(x0, scale_factor=2, mode='nearest') # upsample\n",
    "        #x0 = F.interpolate(x0, scale_factor=2, mode='bilinear', align_corners=True) # upsample\n",
    "        x0 = self.snconv2d0(x0)\n",
    "\n",
    "        out = x + x0\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator.\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, g_conv_dim, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.g_conv_dim = g_conv_dim\n",
    "\n",
    "        self.snlinear0 = snlinear(in_features=z_dim, out_features=g_conv_dim*8*4*4)\n",
    "        \n",
    "        self.block1 = GenBlock(g_conv_dim*8, g_conv_dim*8, num_classes)\n",
    "        self.block2 = GenBlock(g_conv_dim*8, g_conv_dim*4, num_classes)\n",
    "        self.block3 = GenBlock(g_conv_dim*4, g_conv_dim*2, num_classes)\n",
    "        \n",
    "        self.self_attn = Self_Attn(g_conv_dim*2)\n",
    "        \n",
    "        self.block4 = GenBlock(g_conv_dim*2, g_conv_dim, num_classes)\n",
    "        \n",
    "        #self.attn1 = Self_Attn(g_conv_dim*4)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(g_conv_dim, eps=1e-5, momentum=0.0001, affine=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "   \n",
    "        self.snconv2d1 = snconv2d(in_channels=g_conv_dim, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Weight init\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        #z = self.pixelnorm(z)\n",
    "        \n",
    "        # n x z_dim -> n x g_conv_dim*8*4*4(8192)\n",
    "        act0 = self.snlinear0(z)          \n",
    "        \n",
    "        # n x g_conv_dim*8*4*4(8192) -> n x g_conv_dim*8 x 4 x 4(n x 512 x 4 x 4)\n",
    "        act0 = act0.view(-1, self.g_conv_dim*8, 4, 4) \n",
    "        \n",
    "        # n x 512 x 4 x 4 ->  x 512 x 8 x 8\n",
    "        act1 = self.block1(act0, labels)    \n",
    "        \n",
    "        # n x 512 x 8 x 8 -> n x 256 x 16 x 16\n",
    "        act2 = self.block2(act1, labels)    \n",
    "        \n",
    "        #act2 = self.attn1(act2)\n",
    "        \n",
    "        # n x 256 x 16 x 16 -> n x 128 x 32 x 32\n",
    "        act3 = self.block3(act2, labels)    \n",
    "        \n",
    "        # n x 128 x 32 x 32 -> n x 128 x 32 x 32\n",
    "        act3 = self.self_attn(act3)         \n",
    "        \n",
    "        # n x 128 x 32 x 32 -> n x 64 x 64 x 64\n",
    "        act4 = self.block4(act3, labels)    \n",
    "        \n",
    "        #act4 = self.attn2(act4)\n",
    "        #act5 = self.block5(act4, labels)    \n",
    "        \n",
    "        act4 = self.bn(act4)                \n",
    "        act4 = self.relu(act4)              \n",
    "        \n",
    "        \n",
    "        # n x 64 x 64 x 64 -> n x 3 x 64 x 64\n",
    "        act5 = self.snconv2d1(act4)   \n",
    "        \n",
    "        #act5 = self.pixelnorm(act5)\n",
    "        \n",
    "        #  n x 3 x 64 x 64\n",
    "        act5 = self.tanh(act5)              \n",
    "        return act5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscOptBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DiscOptBlock, self).__init__()\n",
    "        self.snconv2d1 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.snconv2d2 = snconv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.downsample = nn.AvgPool2d(2)\n",
    "        self.snconv2d0 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "\n",
    "        x = self.snconv2d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.snconv2d2(x)\n",
    "        x = self.downsample(x)\n",
    "\n",
    "        x0 = self.downsample(x0)\n",
    "        x0 = self.snconv2d0(x0)\n",
    "\n",
    "        out = x + x0\n",
    "        return out\n",
    "\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DiscBlock, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.snconv2d1 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.snconv2d2 = snconv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.downsample = nn.AvgPool2d(2)\n",
    "        self.ch_mismatch = False\n",
    "        if in_channels != out_channels:\n",
    "            self.ch_mismatch = True\n",
    "        self.snconv2d0 = snconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, downsample=True):\n",
    "        x0 = x\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.snconv2d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.snconv2d2(x)\n",
    "        if downsample:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        if downsample or self.ch_mismatch:\n",
    "            x0 = self.snconv2d0(x0)\n",
    "            if downsample:\n",
    "                x0 = self.downsample(x0)\n",
    "\n",
    "        out = x + x0\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, d_conv_dim, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.d_conv_dim = d_conv_dim\n",
    "        self.opt_block1 = DiscOptBlock(3, d_conv_dim)\n",
    "        self.block1 = DiscBlock(d_conv_dim, d_conv_dim*2)\n",
    "        self.self_attn = Self_Attn(d_conv_dim*2)\n",
    "        self.block2 = DiscBlock(d_conv_dim*2, d_conv_dim*4)\n",
    "        self.block3 = DiscBlock(d_conv_dim*4, d_conv_dim*8)\n",
    "        self.block5 = DiscBlock(d_conv_dim*8, d_conv_dim*8)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.snlinear1 = snlinear(in_features=d_conv_dim*8, out_features=1)\n",
    "        self.sn_embedding1 = sn_embedding(num_classes, d_conv_dim*8)\n",
    "\n",
    "        # Weight init\n",
    "        self.apply(init_weights)\n",
    "        xavier_uniform_(self.sn_embedding1.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # x = n x 3 x 64 x 64\n",
    "        # n x 3 x 64 x 64  ->  n x d_conv_dim   x 32 x 32\n",
    "        h0 = self.opt_block1(x) \n",
    "        \n",
    "        # n x d_conv_dim x 32 x 32  ->  n x d_conv_dim*2 x 16 x 16\n",
    "        h1 = self.block1(h0)    \n",
    "        \n",
    "        # n x d_conv_dim*2 x 16 x 16    n x d_conv_dim*2 x 16 x 16\n",
    "        h1 = self.self_attn(h1) \n",
    "        \n",
    "        # n x d_conv_dim*2 x 16 x 16 -> n x d_conv_dim*4 x  8 x  8\n",
    "        h2 = self.block2(h1)    \n",
    "        \n",
    "        # n x d_conv_dim*4 x 8 x 8 -> n x d_conv_dim*8 x 4 x 4\n",
    "        h3 = self.block3(h2)    \n",
    "        \n",
    "        #h4 = self.block4(h3)    # n x d_conv_dim*16 x 4 x  4\n",
    "        \n",
    "        # n x d_conv_dim*8 x 4 x 4 -> # n x d_conv_dim*8 x 4 x 4\n",
    "        h5 = self.block5(h3, downsample=False)  \n",
    "        h5 = self.relu(h5)   \n",
    "        \n",
    "        # n x d_conv_dim*8 x 4 x 4 -> n x d_conv_dim*8\n",
    "        h6 = torch.sum(h5, dim=[2,3])   \n",
    "        \n",
    "        # n x d_conv_dim*8 -> n x 1\n",
    "        output1 = torch.squeeze(self.snlinear1(h6)) \n",
    "        \n",
    "        # Projection  -> n x d_conv_dim*8\n",
    "        h_labels = self.sn_embedding1(labels)   \n",
    "        \n",
    "        # n x d_conv_dim*8 -> n x d_conv_dim*8\n",
    "        proj = torch.mul(h6, h_labels)         \n",
    "        \n",
    "        # n x d_conv_dim*8 -> n x 1\n",
    "        output2 = torch.sum(proj, dim=[1])      \n",
    "        \n",
    "        # Out: n x 1 \n",
    "        output = output1 + output2              \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "### self.build_models()\n",
    "G = Generator(args.z_dim, args.g_conv_dim, num_of_classes).to(device)\n",
    "D = Discriminator(args.d_conv_dim, num_of_classes).to(device)\n",
    "\n",
    "G_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), args.g_lr, [args.beta1, args.beta2])\n",
    "D_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), args.d_lr, [args.beta1, args.beta2])\n",
    "\n",
    "lr_schedulerG = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(G_optimizer, T_0=args.num_epoches//10, eta_min=0.00001)\n",
    "lr_schedulerD = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(D_optimizer, T_0=args.num_epoches//10, eta_min=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.adv_loss == 'dcgan':\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "cudnn.benchmark = True\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "label = torch.full((args.batch_size,), 1, device=device)\n",
    "ones = torch.full((args.batch_size,), 1, device=device)\n",
    "\n",
    "gen_losses = []\n",
    "dis_losses = []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(args.num_epoches):\n",
    "        \n",
    "    for i, (real_images, dog_labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        \n",
    "        real_images = real_images.to(device)\n",
    "        dog_labels = torch.tensor(dog_labels, device=device)\n",
    "                \n",
    "        # =================================== TRAIN D ============================== #\n",
    "        G_optimizer.zero_grad()\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # TRAIN with REAL        \n",
    "        # Get D output for real images & real labels\n",
    "        d_out_real = D(real_images, dog_labels)\n",
    "        \n",
    "        \n",
    "        # Compute D loss with real images & real_labels\n",
    "        label.fill_(args.real_label_value) +  np.random.uniform(-0.1, 0.1)\n",
    "        d_loss_real = criterion(torch.sigmoid(d_out_real), label)\n",
    "        \n",
    "        # Backward\n",
    "        d_loss_real.backward()\n",
    "        \n",
    "        # Train with FAKE        \n",
    "        # Create random noise\n",
    "        z = torch.randn(args.batch_size, args.z_dim, device=device)\n",
    "        \n",
    "        # Generate fake images for same dog labels\n",
    "        fake_images = G(z, dog_labels)\n",
    "        \n",
    "        # Get D output for fake images & same dog labels\n",
    "        d_out_fake = D(fake_images.detach(), dog_labels)\n",
    "        \n",
    "        # Compute D loss with fake images & real labels\n",
    "        label.fill_(args.fake_label_value) + np.random.uniform(0, 0.2)\n",
    "        d_loss_fake = criterion(torch.sigmoid(d_out_fake), label)\n",
    "        \n",
    "        # Backward\n",
    "        d_loss_fake.backward()\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # ====================================== TRAIN G ============================== #\n",
    "        G.zero_grad()\n",
    "        \n",
    "        # Get D output for fake images & same dog labels\n",
    "        d_out_fake = D(fake_images, dog_labels)\n",
    "        \n",
    "        # Compute G loss with fake images & dog_labels\n",
    "        label.fill_(args.real_label_value)\n",
    "        g_loss = criterion(torch.sigmoid(d_out_fake), label)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        \n",
    "        G_optimizer.step()\n",
    "        \n",
    "        gen_losses.append(g_loss.item())\n",
    "        dis_losses.append(d_loss.item())\n",
    "                \n",
    "        lr_schedulerG.step(epoch)\n",
    "        lr_schedulerD.step(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(gen_losses,label=\"G\")\n",
    "plt.plot(dis_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(size, threshold=1):\n",
    "    values = truncnorm.rvs(-threshold, threshold, size=size)\n",
    "    return values\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generate and show samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show generated samples\n",
    "z = truncated_normal((args.batch_size, args.z_dim), threshold=1)\n",
    "gen_z = torch.from_numpy(z).float().to(device)\n",
    "dog_labels = torch.squeeze(torch.randint(0, num_of_classes, (args.batch_size, ), device=device))\n",
    "gen_images = denorm(G(gen_z, dog_labels).to(\"cpu\").clone().detach())\n",
    "gen_images = gen_images.numpy().transpose(0, 2, 3, 1)\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(gen_images[:32]):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generate Submissions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def save_images(generator_model, sample_images_path, submission_dir, z_dim, num_images=10000):\n",
    "    sample_images_dir = Path(sample_images_path)\n",
    "    sample_images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    im_batch_size = 50\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    for i_batch in range(0, num_images, im_batch_size):\n",
    "        z = truncated_normal((im_batch_size, z_dim), threshold=1)\n",
    "        gen_z = torch.from_numpy(z).float().to(device)\n",
    "        dog_labels = torch.squeeze(torch.randint(0, num_of_classes, (im_batch_size, ), device=device))\n",
    "        gen_images = generator_model(gen_z, dog_labels)\n",
    "        images = gen_images.to('cpu').clone().detach()\n",
    "        images = images.numpy().transpose(0, 2, 3, 1)\n",
    "        for i_image in range(gen_images.size(0)):\n",
    "            save_image(denorm(gen_images[i_image, :, :, :]), sample_images_dir / f'image_{i_batch + i_image:05d}.png')\n",
    "    \n",
    "    \n",
    "    submission_dir = Path(submission_dir)\n",
    "    submission_dir.mkdir(exist_ok=True)\n",
    "    shutil.make_archive(os.path.join(submission_dir, 'images'), 'zip', sample_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(generator_model=G, sample_images_path=args.sample_images_path, submission_dir=args.submission_dir, z_dim=args.z_dim, num_images=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('../output_images/images.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dir = Path(args.submission_dir)\n",
    "submission_dir.mkdir(exist_ok=True)\n",
    "shutil.make_archive(os.path.join(args.submission_dir, 'images'), 'zip', args.sample_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./submission/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
